{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài tập 6\n",
    "\n",
    "---\n",
    "Mục tiêu chính ở đây là <font color=green>học, học một cách chân thật</font>; nếu bạn được 5 điểm nhưng bạn làm bài một cách chân thật thì vẫn tốt hơn nhiều so với 10 điểm mà không chân thật. Bạn có thể thảo luận với các bạn khác (nên tận dụng moodle), nhưng <font color=green>bài làm và code phải là của chính bạn, dựa trên sự hiểu của chính bạn</font>. <font color=red>Nếu vi phạm thì bạn sẽ bị 0 điểm cho toàn bộ môn học</font>.\n",
    "\n",
    "Một số lời khuyên khác:\n",
    "- Nên bắt đầu làm sớm, đừng đợi đến gần deadline.\n",
    "- Nên in các file bài tập của Caltech ra (trong môn học, mình sẽ làm 6 bài tập đầu tiên).\n",
    "- Làm từ từ, cẩn thận. Chậm là nhanh, nhanh là chậm.\n",
    "- Tránh xa các nguồn gây nhiễu; ví dụ, facebook ;-)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đề bài "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn download file bài tập \"Homework 6\" (gồm 10 câu hỏi trắc nghiệm) và file đáp án \"Solution key 6\" :-) [ở đây](http://work.caltech.edu/homeworks.html). Với mỗi câu, bạn cần giải thích <font color=blue>tại sao bạn lại chọn đáp án này</font> (không được dùng phương pháp loại trừ). Với những câu mà phải code, bạn cố gắng viết code rõ ràng, dễ đọc (đặt tên biến gợi nhớ, comment những chỗ cần comment). File đáp án ở đây đóng vai trò \"correct output\", giúp bạn biết được là mình đã làm đúng hay chưa, để \"điều chỉnh lại bộ trọng số\". <font color=blue>Bạn nên tự mình làm trước rồi sau đó mới check file đáp án; như vậy sẽ tốt hơn cho việc học của bạn</font>.\n",
    "\n",
    "Bạn sẽ làm bài trên file notebook \"HW6-Submission.ipynb\" mà mình đã cung cấp sẵn. Đầu tiên, bạn điền MSSV và họ tên vào đầu file. Với mỗi câu, mình đã để sẵn một markdown cell mà có ghi là \"YOUR ANSWER HERE\" để bạn làm (khi làm thì bạn có thể xóa dòng \"YOUR ANSWER HERE\" đi). Với mỗi câu, đầu tiên bạn sẽ trình bày các giải thích, và cuối cùng thì chốt lại là bạn chọn đáp án nào (ví dụ, bạn có thể ghi là \"Do đó, em chọn đáp án [d] abcxyz\"). Với mỗi câu, nếu cần thì bạn có thể chèn thêm các cell (code/markdown) ở <font color=blue>ngay trên</font> (không chèn dưới) cell \"YOUR ANSWER HERE\". Ví dụ:\n",
    "\n",
    "Câu 1 (1 điểm)\n",
    "- Cell bạn tự chèn\n",
    "- ...\n",
    "- Cell bạn tự chèn\n",
    "- Cell \"YOUR ANSWER HERE\" (ở cell này ít nhất là có dòng chốt về đáp án bạn chọn)\n",
    "\n",
    "Mình qui định như trên là vì khi chấm bài mình sẽ dùng một chương trình để hỗ trợ quá trình duyệt qua các bài, để có thể chấm nhanh hơn. Mặc dù bạn thấy cell \"YOUR ANSWER HERE\" không có gì đặc biệt, nhưng thật ra là cell này có những thông tin ẩn bên dưới để chương trình hỗ trợ chấm của mình có thể sử dụng. Do đó, <font color=red>bạn lưu ý tuân thủ đúng qui định của mình</font>.\n",
    "\n",
    "**Nộp bài:**\n",
    "\n",
    "Khi chấm bài, đầu tiên mình sẽ chọn \"Kernel\" - \"Restart & Run All\" để restart và chạy tất cả các cell trong notebook của bạn; do đó, trước khi nộp bài, bạn nên chạy thử \"Kernel\" - \"Restart & Run All\" để đảm bảo mọi chuyện diễn ra đúng như mong đợi.\n",
    "\n",
    "Nếu không có vấn đề gì thì bạn tạo thư mục nộp bài theo cấu trúc sau:\n",
    "- Thư mục MSSV (ví dụ, nếu MSSV của bạn là 1234567 thì bạn đặt tên thư mục là 1234567):\n",
    "    - File \"HW6-Submission.ipynb\" (không cần nộp file \"in.dta\" và \"out.dta\"\n",
    "\n",
    "Sau đó, bạn nén thư mục MSSV lại và nộp ở link trên moodle. Đuôi của file nén phải là .zip (chứ không được .rar hay gì khác).\n",
    "\n",
    "Một lần nữa, <font color=red>bạn lưu ý tuân thủ chính xác qui định nộp bài này để chương trình hỗ trợ chấm của mình có thể chạy được</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hướng dẫn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn nên tự làm trước, rồi sau đó mới xem hướng dẫn; mình nghĩ như vậy là tốt nhất cho việc học của bạn. Một câu thì có thể có nhiều cách giải, bạn không nhất thiết là phải làm theo cách mà mình hướng dẫn.\n",
    "\n",
    "Không phải tất cả các câu đều sẽ có hướng dẫn (phần hướng dẫn chủ yếu tập trung vào các câu liên quan đến lập trình). Nếu vẫn có thắc mắc gì khác thì bạn có thể post lên moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vbePR01xbBT"
   },
   "source": [
    "### Câu 1: Overfitting and Deterministic Noise\n",
    "\n",
    "Định nghĩa của deterministic noise (slide của Thầy Yaser, lecture 11 - overfitting, trang 16): \"The part of $f$ that $\\mathcal H$ cannot capture: $f(\\mathbf{x}) − h^∗(\\mathbf{x})$\". Trong đó, $h^*(\\mathbf{x})$ là hàm tốt nhất của $\\mathcal H$ dùng để xấp xỉ $f$.\n",
    "\n",
    "\"Deterministic noise\" sẽ thay đổi như thế nào nếu dùng $\\mathcal H' \\subset  \\mathcal H$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9cVt_qZ0Gywa"
   },
   "source": [
    "### Câu 2-6: Regularization with Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đọc dữ liệu từ file vào mảng Numpy**\n",
    "\n",
    "Bạn có thể thực hiện việc này một cách nhanh chóng bằng hàm `np.loadtxt`. Ví dụ, để đọc file \"in.dta\" vào mảng \"train_data\":\n",
    "\n",
    "    import numpy as np\n",
    "    train_data = np.loadtxt(\"in.dta\")\n",
    "    \n",
    "Kết quả `train_data` sẽ là một mảng Numpy 2 chiều, có 3 cột, trong đó: cột cuối ứng với các output $y$, các cột còn lại ứng với các véc-tơ input $\\mathbf{x}$. Từ mảng `train_data` này, bạn có thể tách ra mảng `train_X` và `train_Y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression cho classification**\n",
    "\n",
    "Như đã học:\n",
    "- Khi huấn luyện, ta sẽ xem output $y=\\pm 1$ là số thực và tìm $w$ một cách nhanh chóng bằng công thức ở slide 11, lecture 12 - regularization. Bạn có thể chỉ cần cài đặt công thức tính $\\mathbf{w_{reg}}$, khi không dùng regularization thì cho $\\lambda=0$. Trong công thức tính $\\mathbf{w_{reg}}$, $\\mathbf{I}$ là ma trận đơn vị (ma trận vuông, các phần tử trên đường chéo chính bằng 1, còn lại là 0). Trong Numpy, để tạo ra một ma trận đơn vị, giả sử có kích thước $5\\times5$, thì bạn có thể dùng câu lệnh: `np.eye(5)`. Trong công thức tính $\\mathbf{w_{reg}}$ thì $\\mathbf{I}$ là ma trận đơn vị có kích thước bằng với kích thước của $\\mathbf{Z}^T\\mathbf{Z}$.\n",
    "- Sau khi đã có $\\mathbf{w}$, để dự đoán lớp ($\\pm1$) của một véc-tơ input $\\mathbf{x}$ thì dùng công thức: $sign(\\mathbf{w}^T\\mathbf{x})$. Để ý $E_{in}$ và $E_{out}$ mà đề bài hỏi là độ lỗi nhị phân khi phân lớp (chứ không phải độ lỗi bình phương khi hồi qui)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ảnh hưởng của siêu tham số $\\lambda$**\n",
    "\n",
    "Như đã học, $\\lambda$ cho biết mức độ kìm hãm khả năng fit của mô hình. $\\lambda$ càng lớn thì mô hình sẽ càng bị kìm hãm khả năng fit, $E_{in}$ nói chung sẽ càng lớn, còn $E_{out}$ thì sao? Nếu khi chưa dùng regularization ($\\lambda=0$) mà mô hình đã bị overfit thì khi tăng $\\lambda$ nói chung sẽ giúp mô hình đỡ fit phải nhiễu và do đó giúp $E_{out}$ giảm xuống; nhưng khi tăng $\\lambda$ quá một mức nào đó thì sẽ làm mô hình không đủ khả năng để fit được cái nên fit và do đó $E_{out}$ lại tăng lên. Còn nếu khi chưa dùng regularization ($\\lambda=0$) mà mô hình không bị overfit thì khi tăng $\\lambda$ lên sẽ không giúp làm giảm $E_{out}$.\n",
    "\n",
    "Ở câu 5 và 6, khi bạn thí nghiệm với $\\lambda$ tăng dần lên thì có thể sẽ không thấy một cách rõ ràng điều mình mô tả ở trên. Lý do là vì: khi học, độ lỗi mà Linear Regression trực tiếp cực tiểu hóa là độ lỗi bình phương nhưng mà độ lỗi in ra xem lại là độ lỗi nhị phân (sau khi lấy sign của $\\mathbf{w}^T\\mathbf{x}$). Bạn thử in ra thêm độ lỗi bình phương (không lấy sign của $\\mathbf{w}^T\\mathbf{x}$) và xem có giống điều mình mô tả ở trên khi tăng $\\lambda$ không."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EswJI0uBGywc"
   },
   "source": [
    "### Câu 7: Regularization for Polynomials\n",
    "\n",
    "Câu này mình nghĩ là bạn có thể tự làm được. \n",
    "\n",
    "Mình chỉ muốn nói thêm một ít về phép transform được nói đến trong đề bài:\n",
    "$$x \\in \\mathbb{R} \\rightarrow \\mathbf{z} \\in \\mathbb{R}^{Q+1}$$\n",
    "với $\\mathbf{z}$ gồm các thành phần: $L_0(x)=0, L_1(x), L_2(x), ..., L_Q(x)$. Trong đó $L_q(x)$ là đa thức Legendre bậc $q$; bạn có thể xem một số đa thức Legendre ở slide của Thầy Yaser, lecture 12 - regularization, trang 6. Tập hypothesis của Linear Regression trong không gian $\\mathcal{Z}$ theo phép biến đổi ở trên là tập các đa thức bậc $Q$. \n",
    "\n",
    "Một cách khác là dùng Linear Regression trong không gian $\\mathcal{Z}$ có được bằng phép biến đổi: $x \\rightarrow \\mathbf{z} = [1, x, x^2, ..., x^Q]^T$. 2 tập hypothesis ứng với 2 cách làm này là tương đương nhau và đều là tập đa thức bậc $Q$. \n",
    "\n",
    "Điểm khác nhau mà mình thấy là: ở cách làm thứ 2, các thành phần trong véc-tơ $\\mathbf{z}$ có range khác nhau và dẫn đến các trọng số $w$ tương ứng cũng sẽ có range khác nhau (ví dụ, nếu $x \\in [-1, 1]$ thì $x^q$ sẽ có range càng nhỏ khi $q$ càng lớn, và $w_q$ tương ứng sẽ có range càng lớn); vụ range khác nhau này có thể sẽ gây ra một số vấn đề (chẳng hạn, nếu dùng regularization $\\sum_i w_i^2$ thì việc các $w_i$ có range khác nhau sẽ làm cho một số $w_i$ bị tập trung kìm hãm hơn, đây có thể là điều ta không muốn). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_BUaPhoGywe"
   },
   "source": [
    "### Câu 8-10: Neural Networks\n",
    "\n",
    "**Câu 8:**\n",
    "\n",
    "Một lần backpropagation với một mẫu dữ liệu $(\\mathbf{x}, y)$ mà đề bài hỏi gồm các bước:\n",
    "1. Bắt đầu từ véc-tơ input $\\mathbf{x}$, thực hiện lan truyền tiến qua các tầng của mạng để tính ra các giá trị đầu ra $a$ của các nơ-ron. Lưu ý: ở đây mình dùng ký hiệu $a^{(l)}_i$ để chỉ giá trị đầu ra của nơ-ron $i$ ở tầng $l$, còn trong đề bài là dùng ký hiệu $x^{(l)}_i$. Có bao nhiêu phép tính $w^{(l)}_{ij}a^{(l-1)}_i$ ở bước lan truyền tiến này? (lưu ý: $d^{(l)}$ là số lượng nơ-ron ở tầng $l$ mà chưa tính nơ-ron luôn có giá trị $+1$.)\n",
    "2. Từ $\\delta$ ở tầng cuối, thực hiện lan truyền ngược để tính $\\delta$ của các nơ-ron ở các tầng trước đó. Có bao nhiêu phép tính $w^{(l)}_{ij}\\delta^{(l)}_j$ ở bước này? Gợi ý: mục tiêu của việc tính các $\\delta$ là để tính các đạo hàm riêng (ở bước 3), sẽ có một số nơ-ron sẽ không cần tính $\\delta$ vì nếu tính thì không dùng để làm gì cả.\n",
    "3. Với mỗi trọng số $w^{(l)}_{ij}$, tính $\\frac{\\partial e}{\\partial w^{(l)}_{ij}}$ bằng cách lấy $a$ ở một đầu (nơ-ron $i$ ở tầng $l-1$) nhân với $\\delta$ ở đầu kia (nơ-ron $j$ ở tầng $l$). Có bao nhiêu phép tính $a^{(l-1)}_i\\delta^{(l)}_j$ ở bước này?\n",
    "\n",
    "<font color=green>\"Correct output\" của mình: 47.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_BUaPhoGywe"
   },
   "source": [
    "**Câu 9, 10:**\n",
    "\n",
    "Bạn có thể lập luận hoặc viết một chương trình xét tất cả các trường hợp. Lưu ý: ở mỗi tầng ẩn phải có một nơ-ron \"+1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
