{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài tập 5\n",
    "\n",
    "---\n",
    "Mục tiêu chính ở đây là <font color=green>học, học một cách chân thật</font>; nếu bạn được 5 điểm nhưng bạn làm bài một cách chân thật thì vẫn tốt hơn nhiều so với 10 điểm mà không chân thật. Bạn có thể thảo luận với các bạn khác (nên tận dụng moodle), nhưng <font color=green>bài làm và code phải là của chính bạn, dựa trên sự hiểu của chính bạn</font>. <font color=red>Nếu vi phạm thì bạn sẽ bị 0 điểm cho toàn bộ môn học</font>.\n",
    "\n",
    "Một số lời khuyên khác:\n",
    "- Nên bắt đầu làm sớm, đừng đợi đến gần deadline.\n",
    "- Nên in các file bài tập của Caltech ra (trong môn học, mình sẽ làm 6 bài tập đầu tiên).\n",
    "- Làm từ từ, cẩn thận. Chậm là nhanh, nhanh là chậm.\n",
    "- Tránh xa các nguồn gây nhiễu; ví dụ, facebook ;-)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đề bài "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn download file bài tập \"Homework 5\" (gồm 10 câu hỏi trắc nghiệm) và file đáp án \"Solution key 5\" :-) [ở đây](http://work.caltech.edu/homeworks.html). Với mỗi câu, bạn cần giải thích <font color=blue>tại sao bạn lại chọn đáp án này</font> (không được dùng phương pháp loại trừ). Với những câu mà phải code, bạn cố gắng viết code rõ ràng, dễ đọc (đặt tên biến gợi nhớ, comment những chỗ cần comment). File đáp án ở đây đóng vai trò \"correct output\", giúp bạn biết được là mình đã làm đúng hay chưa, để \"điều chỉnh lại bộ trọng số\". <font color=blue>Bạn nên tự mình làm trước rồi sau đó mới check file đáp án; như vậy sẽ tốt hơn cho việc học của bạn</font>.\n",
    "\n",
    "Bạn sẽ làm bài trên file notebook \"HW5-Submission.ipynb\" mà mình đã cung cấp sẵn. Đầu tiên, bạn điền MSSV và họ tên vào đầu file. Với mỗi câu, mình đã để sẵn một markdown cell mà có ghi là \"YOUR ANSWER HERE\" để bạn làm (khi làm thì bạn có thể xóa dòng \"YOUR ANSWER HERE\" đi). Với mỗi câu, đầu tiên bạn sẽ trình bày các giải thích, và cuối cùng thì chốt lại là bạn chọn đáp án nào (ví dụ, bạn có thể ghi là \"Do đó, em chọn đáp án [d] abcxyz\"). Với mỗi câu, nếu cần thì bạn có thể chèn thêm các cell (code/markdown) ở <font color=blue>ngay trên</font> (không chèn dưới) cell \"YOUR ANSWER HERE\". Ví dụ:\n",
    "\n",
    "Câu 1 (1 điểm)\n",
    "- Cell bạn tự chèn\n",
    "- ...\n",
    "- Cell bạn tự chèn\n",
    "- Cell \"YOUR ANSWER HERE\" (ở cell này ít nhất là có dòng chốt về đáp án bạn chọn)\n",
    "\n",
    "Mình qui định như trên là vì khi chấm bài mình sẽ dùng một chương trình để hỗ trợ quá trình duyệt qua các bài, để có thể chấm nhanh hơn. Mặc dù bạn thấy cell \"YOUR ANSWER HERE\" không có gì đặc biệt, nhưng thật ra là cell này có những thông tin ẩn bên dưới để chương trình hỗ trợ chấm của mình có thể sử dụng. Do đó, <font color=red>bạn lưu ý tuân thủ đúng qui định của mình</font>.\n",
    "\n",
    "**Nộp bài:**\n",
    "\n",
    "Khi chấm bài, đầu tiên mình sẽ chọn \"Kernel\" - \"Restart & Run All\" để restart và chạy tất cả các cell trong notebook của bạn; do đó, trước khi nộp bài, bạn nên chạy thử \"Kernel\" - \"Restart & Run All\" để đảm bảo mọi chuyện diễn ra đúng như mong đợi.\n",
    "\n",
    "Nếu không có vấn đề gì thì bạn tạo thư mục nộp bài theo cấu trúc sau:\n",
    "- Thư mục MSSV (ví dụ, nếu MSSV của bạn là 1234567 thì bạn đặt tên thư mục là 1234567):\n",
    "    - File \"HW5-Submission.ipynb\"\n",
    "\n",
    "Sau đó, bạn nén thư mục MSSV lại và nộp ở link trên moodle. Đuôi của file nén phải là .zip (chứ không được .rar hay gì khác).\n",
    "\n",
    "Một lần nữa, <font color=red>bạn lưu ý tuân thủ chính xác qui định nộp bài này để chương trình hỗ trợ chấm của mình có thể chạy được</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hướng dẫn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn nên tự làm trước, rồi sau đó mới xem hướng dẫn; mình nghĩ như vậy là tốt nhất cho việc học của bạn. Một câu thì có thể có nhiều cách giải, bạn không nhất thiết là phải làm theo cách mà mình hướng dẫn.\n",
    "\n",
    "Không phải tất cả các câu đều sẽ có hướng dẫn (phần hướng dẫn chủ yếu tập trung vào các câu liên quan đến lập trình). Nếu vẫn có thắc mắc gì khác thì bạn có thể post lên moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 1: Linear Regression Error\n",
    "Với setup như trong đề bài thì người ta đã chứng minh được:\n",
    "$$\\mathbb{E}_{\\mathcal{D}}[E_{in}(\\mathbf{w}_{lin})] = \\sigma^2\\left(1 - \\frac{d+1}{N}\\right)$$\n",
    "Đầu tiên, mình muốn giải thích một ít để bạn hiểu hơn về công thức này. Cái mà mô hình học thấy là các ouput $y$ tương ứng với các input $\\mathbf{x}$, và trong quá trình học mô hình cố gắng fit vào các output $y$ này. Nhưng output $y$ này bao gồm <font color=green>output \"sạch\" $\\mathbf{w}^{*T}\\mathbf{x}$ (là cái nên fit)</font> + <font color=red>nhiễu $\\epsilon$ có mean 0 và variance $\\sigma^2$ (nhiễu là cái không nên fit)</font>. Nếu bằng một cách nào đó, mô hình có thể fit vào output sạch thì $\\mathbf{w}_{lin}$ học được bằng với $\\mathbf{w^*}$ và $E_{in}\\approx E_{out} = \\sigma^2$ (độ lỗi đang nói tới ở đây là độ lỗi bình phương); đây là trường hợp tốt nhất. Theo công thức trên, ta thấy khi $N$ tăng thì $\\frac{d+1}{N}$ sẽ giảm về 0 và $E_{in}$ sẽ tăng lên và tiến tới $\\sigma^2$. Điều này là hợp lý vì khi $N$ nhỏ, mô hình có thể fit vào nhiễu và làm cho $E_{in}$ nhỏ hơn $\\sigma^2$ (tuy nhiên, đây không phải là điều tốt vì $E_{out}$ lớn hơn $\\sigma^2$; bạn có thể xem thêm hình vẽ ở slide của Thầy Yaser ở lecture 8 - bias variance, trang 22). Khi $N$ tăng, mô hình sẽ dần thấy được sự thật: nhiễu là cái không thể fit (bạn hình dung khi một $\\mathbf{x}$ bắt đầu có nhiều $y$ thì mô hình chỉ có thể fit vào giá trị trung bình ở giữa); và bắt đầu fit vào cái nên fit là $\\mathbf{w}^{*T}\\mathbf{x}$. Lúc này, $E_{in}$ sẽ tăng lên và tiến tới $\\sigma^2$ (tuy nhiên, đây là điều tốt vì $E_{out}$ giảm và tiến tới $\\sigma^2$).\n",
    "\n",
    "Câu này hỏi là với $\\sigma=0.1$ và $d=8$ thì cần $N$ nhỏ nhất là bao nhiêu để $E_{in}$ (theo công thức ở trên) lớn hơn 0.008. Đây là bất phương trình theo $N$ và có thể giải được một cách dễ dàng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 2-3: Nonlinear Transforms\n",
    "**Câu 2:** \n",
    "\n",
    "Như đã học, nonlinear transform là một cách để tăng sức mạnh cho các mô hình tuyến tính, giúp các mô hình tuyến tính biểu diễn được các \"đường cong\" (bình thường thì chỉ biểu diễn được các \"đường thẳng\"). Phép nonlinear transform được cho trong đề bài là phép ánh xạ từ không gian $\\mathcal{X}$ hai chiều (chưa tính $x_0=1$) sang không gian $\\mathcal{Z}$ hai chiều (chưa tính $z_0=1$): $$\\Phi(1, x_1, x_2) = (1, x_1^2, x_2^2)$$\n",
    "Với phép transform này thì tập hypothesis của mô hình phân lớp tuyến tính (Perceptron) sẽ gồm các hypothesis có dạng: $$h(\\mathbf{x})=sign(\\tilde{w}_0 + \\tilde{w}_1 x_1^2 + \\tilde{w}_2 x_2^2)$$ \n",
    "Ví dụ, nếu $\\tilde{w}_0 = -4$, $\\tilde{w}_1=1$, $\\tilde{w}_2=1$ thì có thể dễ thấy $h(x)$ sẽ ứng với một đường tròn có bán kính bằng 2 trong không gian $\\mathcal{X}$, ngoài đường tròn thì $h(\\mathbf{x})=+1$ và trong đường tròn thì $h(\\mathbf{x})=-1$.\n",
    "\n",
    "Với các giá trị khác của $\\mathbf{\\tilde{w}}$ thì đường phân lớp của $h(x)$ có thể có các hình dạng khác, ví dụ như ở hình trong đề bài. Câu hỏi là với $\\mathbf{\\tilde{w}}$ như thế nào thì sẽ ra được hình trong đề bài?\n",
    "\n",
    "Một cách để làm bài này là dùng code để vẽ hình ứng các trường hợp của $\\mathbf{\\tilde{w}}$ và chọn ra trường hợp giống với hình cho trong đề bài. Một cách khác là lập luận như sau:\n",
    "- Ta có: $h(\\mathbf{x}) = sign(\\tilde{w}_0 + \\tilde{w}_1 x_1^2 + \\tilde{w}_2 x_2^2)$\n",
    "- Khi $x_1$ siêu âm hoặc siêu dương thì $x_1^2$ sẽ dương và rất lớn, và  $\\tilde{w}_1 x_1^2$ sẽ là đại lượng quyết định dấu của tổng trong công thức ở trên. Do $x_1^2$ dương nên $\\tilde{w}_1$ sẽ quyết định dấu của $\\tilde{w}_1 x_1^2$.\n",
    "- Tương tự với $x_2$.\n",
    "- Nhìn hình và suy luận ra dấu của $\\tilde{w}_1$ và $\\tilde{w}_2$.\n",
    "\n",
    "**Câu 3:** \n",
    "\n",
    "Bạn sử dụng công thức về $d_{VC}$ của mô hình phân lớp tuyến tính (Perceptron) trong không gian $\\mathcal{Z}$ ở slide của Thầy Yaser, lecture 9 - the linear model II, trang 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 4-7: Gradient Descent\n",
    "\n",
    "Mình nghĩ bạn có thể tự làm được các câu này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 8-9: Logistic Regression\n",
    "\n",
    "Ở đây sẽ tập trung hướng dẫn về hàm quan trọng nhất là hàm huấn luyện (thuật toán học) của mô hình Logistic Regression. Ngoài ra, cũng sẽ nói về hàm dự đoán và hàm tính độ lỗi cross-entropy. Các hàm khác như phát sinh $f$ và phát sinh dữ liệu thì bạn có thể tham khảo trong file \"HW1.ipynb\" ở bài tập 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hàm huấn luyện Logistic Regression**\n",
    "\n",
    "<font color=blue>Input của hàm </font>\n",
    "- `X`: mảng Numpy, kích thước $N\\times(d+1)$ với $N$ là số lượng mẫu huấn luyện, $d$ là số chiều của véc-tơ input (chưa tính thành phần $x_0=1$). Mỗi dòng của `X` ứng với một véc-tơ input (đã có $x_0$).\n",
    "- `Y`: mảng Numpy, kích thước $N\\times1$. `Y` chứa các output đúng tương ứng với các véc-tơ input trong `X`.\n",
    "\n",
    "<font color=blue>Output của hàm</font>\n",
    "\n",
    "- `w`: mảng Numpy, kích thước $(d+1)\\times1$. `w` chính là véc-tơ trọng số của final hypothesis $g$.\n",
    "- `n_epoch`: số nguyên cho biết số lượng epoch đã được thực hiện (một epoch ứng với một lần duyệt qua tập dữ liệu huấn luyện), ta output ra giá trị này vì đề bài có hỏi.\n",
    "\n",
    "<font color=blue>Quá trình thực hiện</font>\n",
    "\n",
    "Ta sẽ tìm `w` bằng cách sử dụng thuật toán SGD (kích thước minibatch bằng 1) để cực tiểu hóa $E_{in}(\\mathbf{w})$. Cụ thể các bước như sau:\n",
    "- Cho `learning_rate` của SGD bằng 0.01 (theo đề bài).\n",
    "- Khởi tạo `w` là mảng Numpy có kích thước $(d+1)\\times1$, trong đó các phần tử có giá trị bằng 0 (gợi ý: bạn có thể dùng hàm `np.zeros`).\n",
    "- Khởi tạo `n_epochs` bằng 0.\n",
    "- Lặp (mỗi vòng lặp ứng với một epoch):\n",
    "    - Trước khi duyệt qua tập dữ liệu huấn luyện và cập nhật mảng `w` thì ta lưu lại nội dung mảng `w` hiện tại vào `previous_w`. Lưu ý: bạn phải dùng hàm `np.copy` để lưu lại nội dung của `w`, bạn xem thêm ở ví dụ trong [document](https://www.numpy.org/devdocs/reference/generated/numpy.copy.html?highlight=copy#numpy.copy).\n",
    "    - Tạo `rand_idxs` là mảng Numpy có kích thước $N$, chứa các giá trị chỉ số $0, 1, ..., N-1$ nhưng được xáo trộn ngẫu nhiên (gợi ý: bạn có thể dùng hàm `np.random.permutation(N)`).\n",
    "    - Với mỗi chỉ số `idx` trong mảng các chỉ số ngẫu nhiên `rand_idxs`:\n",
    "        - Lấy ra véc-tơ input `x` (mảng Numpy, kích thước $(d+1)\\times1$) và giá trị output `y` tương ứng (con số) từ mảng `X` và `Y` dựa vào chỉ số `idx`.\n",
    "        - Tính `gradient` (mảng Numpy, kích thước $(d+1)\\times1$) từ `x`, `y`, `w` theo công thức ở slide của Thầy Yaser, lecture 9 - the linear model II, trang 23. Tuy nhiên, công thức này là tính gradient trung bình trên toàn bộ tập dữ liệu huấn luyện, còn ở đây ta chỉ tính gradient tại một mẫu (`x`, `y`).\n",
    "        - Cập nhật mảng `w` dựa vào `learning_rate` và `gradient`.\n",
    "    - Cập nhật `n_epochs`.\n",
    "    - Kiểm tra điều kiện dừng dựa vào `w` và `previous_w` (xem đề bài). *Nếu thỏa thì thoát ra khỏi vòng lặp!*\n",
    "- `return w, n_epochs`.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test hàm huấn luyện Logistic Regression**\n",
    "\n",
    "Để biết đã cài đặt đúng hàm huấn luyện Logistic Regression hay chưa thì một cách là cho in ra độ lỗi cross-entropy trên tập huấn luyện sau mỗi epoch; độ lỗi này *nhìn chung* là nên đi xuống. Ngoài ra, bạn có thể in ra thêm độ lỗi binary error trên tập huấn luyện sau mỗi epoch; mặc dù độ lỗi cross-entropy là độ lỗi mà ta trực tiếp cực tiểu hóa nhưng khi độ lỗi này đi xuống thì nói chung độ lỗi binary error cũng sẽ đi xuống."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hàm dự đoán** (hàm này bạn có thể sẽ không dùng tới trong bài này nhưng mình cũng hướng dẫn luôn cho đầy đủ)\n",
    "\n",
    "Sau khi học được `w`, bạn có thể dùng `w` này để dự đoán xác suất $y=1$ với các véc-tơ input mới. Vì hàm này cũng dễ (và chắc là không cần dùng trong bài này) nên mình sẽ viết luôn cho bạn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(X, w):\n",
    "    S = X.dot(w)\n",
    "    probs = 1 / (1 + np.exp(-S))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hàm tính độ lỗi cross entropy của `w` trên một tập dữ liệu**\n",
    "\n",
    "Bạn xem công thức ở slide của Thầy Yaser, lecture 9 - the linear model II, trang 16. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Câu 10: PLA as SGD\n",
    "\n",
    "- PLA: \n",
    "    - Với mẫu $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$ được phân lớp đúng thì $\\mathbf{w}$ không thay đổi.\n",
    "    - Với mẫu $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$ bị phân lớp sai thì cập nhật: $\\mathbf{w} \\leftarrow \\mathbf{w} + y^{(n)}\\mathbf{x}^{(n)}$.\n",
    "\n",
    "- SGD: với mỗi mẫu $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$ thì đều cập nhật $\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\times \\mathbf{\\nabla_wE_{in}}$.\n",
    "\n",
    "- Có thể xem PLA là SGD với hàm lỗi $E_{in}$ cần cực tiểu hóa là hàm nào? Ở đây, $E_{in}=\\frac{1}{N}\\sum_{n=1}^Ne_n$ với $e_n$ là độ lỗi tại một mẫu huấn luyện $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$. Đề bài là hỏi về công thức của $e_n$. Gợi ý: cho $\\alpha=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
